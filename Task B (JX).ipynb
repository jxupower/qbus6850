{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ac4fd5a",
   "metadata": {},
   "source": [
    "# Task B\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69df055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing library\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense, Dropout, SpatialDropout1D\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import keras_tuner as kt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, log_loss\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from collections import Counter\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37462256",
   "metadata": {},
   "source": [
    "# 1. Read database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2985c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading  dataset\n",
    "\n",
    "data = pd.read_csv('train.csv')\n",
    "data_realtest = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b304efa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = []\n",
    "my_stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "vocabulary = {}\n",
    "review_size = []\n",
    "\n",
    "def tokenize(text):\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') # remove punctuation and numbers\n",
    "    nopunct = regex.sub(\" \", text.lower())\n",
    "    return nltk.tokenize.word_tokenize(nopunct)\n",
    "\n",
    "\n",
    "for i in range(len(data['rating'])):\n",
    "    review.append(data['reviewText'][i] +' '+ data['summary'][i])\n",
    "\n",
    "counts = Counter()\n",
    "\n",
    "for i in range(len(review)):\n",
    "    r = []\n",
    "    tokens = tokenize(review[i])\n",
    "    for t in tokens:\n",
    "        if t not in my_stopwords and len(t) > 1:\n",
    "            r.append(lemmatizer.lemmatize(t))\n",
    "            \n",
    "    review[i] = r\n",
    "    counts.update(r)\n",
    "    review_size.append(len(r))\n",
    "    \n",
    "data['review'] = review\n",
    "\n",
    "review = []\n",
    "review_size = []\n",
    "\n",
    "for i in range(len(data_realtest['Id'])):\n",
    "    review.append(data_realtest['reviewText'][i] +' '+ data_realtest['summary'][i])\n",
    "\n",
    "counts = Counter()\n",
    "\n",
    "for i in range(len(review)):\n",
    "    r = []\n",
    "    tokens = tokenize(review[i])\n",
    "    for t in tokens:\n",
    "        if t not in my_stopwords and len(t) > 1:\n",
    "            r.append(lemmatizer.lemmatize(t))\n",
    "            \n",
    "    review[i] = r\n",
    "    counts.update(r)\n",
    "    review_size.append(len(r))\n",
    "    \n",
    "data_realtest['review'] = review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d041c0",
   "metadata": {},
   "source": [
    "# 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f74384f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnltk.download('stopwords')\\nnltk.download('wordnet')\\nnltk.download('punkt')\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "640ce258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstop = stopwords.words(\\'english\\')\\n\\ndata[\\'reviewText\\'] = data[\\'reviewText\\'].apply(lambda x: \\' \\'.join([word for word in x.split() if word not in (stop)]))\\ndata[\\'summary\\'] = data[\\'summary\\'].apply(lambda x: \\' \\'.join([word for word in x.split() if word not in (stop)]))\\n\\nw_tokenizer = nltk.tokenize.WhitespaceTokenizer()\\nlemmatizer = nltk.stem.WordNetLemmatizer()\\ndef lemmatize_text(text):\\n    st = \"\"\\n    for w in w_tokenizer.tokenize(text):\\n        st = st + lemmatizer.lemmatize(w) + \" \"\\n    return st\\ndata[\\'reviewText\\'] = data.reviewText.apply(lemmatize_text)\\ndata[\\'summary\\'] = data.summary.apply(lemmatize_text)\\n\\ndata.head()\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "data['reviewText'] = data['reviewText'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "data['summary'] = data['summary'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    st = \"\"\n",
    "    for w in w_tokenizer.tokenize(text):\n",
    "        st = st + lemmatizer.lemmatize(w) + \" \"\n",
    "    return st\n",
    "data['reviewText'] = data.reviewText.apply(lemmatize_text)\n",
    "data['summary'] = data.summary.apply(lemmatize_text)\n",
    "\n",
    "data.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e634d13a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating        0\n",
       "reviewText    0\n",
       "summary       0\n",
       "review        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e81d33c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    2400\n",
       "5    2200\n",
       "1    1700\n",
       "2    1500\n",
       "3    1200\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb003a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nvectorizer = CountVectorizer(stop_words = 'english',min_df=2)\\nreviews = vectorizer.fit_transform(data['reviewText'])\\nsummary = vectorizer.fit_transform(data['summary'])\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "vectorizer = CountVectorizer(stop_words = 'english',min_df=2)\n",
    "reviews = vectorizer.fit_transform(data['reviewText'])\n",
    "summary = vectorizer.fit_transform(data['summary'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f8b857",
   "metadata": {},
   "source": [
    "# 3. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0796440e",
   "metadata": {},
   "source": [
    "1. Preprocess your data so you remove conjunctions, stop words, and \"junk\" from tweets.<br>\n",
    "2. lemmatization<br>\n",
    "3. Analyze the words with the top frequencies. Are these words that convey sentiment? Could they be removed in your preprocessing? The tokenizer records the first N unique words until the dictionary has num_words in it, so these popular words are much more likely to be in your dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87d993b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5400,), (1800,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data into X and y\n",
    "X = data.review\n",
    "y = data.rating\n",
    "X_realtest = data_realtest.review\n",
    "\n",
    "# 60:20:20 split\n",
    "X_tv, X_test, y_tv, y_test = train_test_split(X,y,test_size=0.20,random_state=0)\n",
    "X_train, X_vali, y_train, y_vali = train_test_split(X_tv, y_tv, test_size = 1/4,random_state=0) \n",
    "\n",
    "y_train_array = np.array(y_train)\n",
    "y_vali_array = np.array(y_vali)\n",
    "y_tv_array = np.array(y_tv)\n",
    "y_test_array = np.array(y_test)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b38aae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# One Hot Encode Y values:\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "y_train = encoder.fit_transform(y_train.values)\n",
    "y_train = to_categorical(y_train) \n",
    "\n",
    "y_vali = encoder.fit_transform(y_vali.values)\n",
    "y_vali = to_categorical(y_vali) \n",
    "\n",
    "y_tv = encoder.fit_transform(y_tv.values)\n",
    "y_tv = to_categorical(y_tv) \n",
    "\n",
    "y_test = encoder.fit_transform(y_test.values)\n",
    "y_test = to_categorical(y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "316ac9aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens: 18931\n",
      "Max Token Index: 18931 \n",
      "\n",
      "Sample Before Processing: ['book', 'short', 'even', 'qualify', 'novella', 'guess', 'get', 'pay', 'free', 'seriously']\n",
      "Sample After Processing: ['book short even qualify novella guess get pay free seriously'] \n",
      "\n",
      "What the model will interpret: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 19, 21, 5906, 154, 359, 12, 511, 80, 505]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(lower=False) # num_words:the maximum number of words to keep, based on word frequency\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_vali = tokenizer.texts_to_sequences(X_vali)\n",
    "sequences_tv = tokenizer.texts_to_sequences(X_tv)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "sequences_realtest = tokenizer.texts_to_sequences(X_realtest)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "padded_sequence_train = pad_sequences(sequences_train, maxlen=600) # maxlen, higher num takes longer to run\n",
    "padded_sequence_vali = pad_sequences(sequences_vali, maxlen=600)\n",
    "padded_sequence_tv = pad_sequences(sequences_tv, maxlen=600)\n",
    "padded_sequence_test = pad_sequences(sequences_test, maxlen=600)\n",
    "padded_sequence_realtest = pad_sequences(sequences_realtest, maxlen=600)\n",
    "\n",
    "print('Number of Tokens:', len(tokenizer.word_index))\n",
    "print(\"Max Token Index:\", padded_sequence_train.max(), \"\\n\")\n",
    "\n",
    "print('Sample Before Processing:', X_train.values[0])\n",
    "print('Sample After Processing:', tokenizer.sequences_to_texts([padded_sequence_train[0]]), '\\n')\n",
    "\n",
    "print('What the model will interpret:', padded_sequence_train[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cf0cd45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 600)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequence_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53fe2aa",
   "metadata": {},
   "source": [
    "# 4. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8301af37",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 11m 17s]\n",
      "val_accuracy: 0.4794444441795349\n",
      "\n",
      "Best val_accuracy So Far: 0.5163888931274414\n",
      "Total elapsed time: 08h 40m 34s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "# https://keras.io/keras_tuner/\n",
    "# https://blog.tensorflow.org/2020/01/hyperparameter-tuning-with-keras-tuner.html\n",
    "# https://datascience.stackexchange.com/questions/73605/opinions-on-an-lstm-hyper-parameter-tuning-process-i-am-using\n",
    "# https://www.youtube.com/watch?v=vvC15l4CY1Q\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim = vocab_size, output_dim = 128, input_length=600))\n",
    "    model.add(LSTM(units=hp.Int('units_LSTM',min_value=16,max_value=256,step=16)))\n",
    "    # Tune the number of dense layers\n",
    "    for i in range(hp.Int('num_layers', 1, 3)):\n",
    "        model.add(Dense(units=hp.Int('units_'+str(i), min_value=16, max_value=256, step=16), activation=\"relu\"))    \n",
    "        model.add(Dropout(hp.Float('rate_'+str(i), min_value=0.0, max_value=0.5, step=0.1)))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3])\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer='adam')\n",
    "\n",
    "    return model\n",
    "\n",
    "tuner = kt.RandomSearch(build_model,objective=\"val_accuracy\", max_trials=5,executions_per_trial=2,overwrite=True)\n",
    "\n",
    "tuner.search(padded_sequence_train, y_train, validation_data=(padded_sequence_vali, y_vali),\n",
    "                    epochs = 3,\n",
    "                    batch_size=64)\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "085adc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "240\n",
      "2\n",
      "128\n",
      "0.4\n",
      "16\n",
      "0.0\n",
      "0.01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "{best_hps.get('units_LSTM')}\n",
    "{best_hps.get('num_layers')}\n",
    "{best_hps.get('units_0')}\n",
    "{best_hps.get('rate_0')}\n",
    "{best_hps.get('units_1')}\n",
    "{best_hps.get('rate_1')}\n",
    "{best_hps.get('learning_rate')}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08ab554b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    if hp.Boolean(\"dropout\"):\\n        model.add(layers.Dropout(rate=0.25))\\n    model.add(Dense(units=hp.Int(\\'units_2\\',min_value=32, max_value=256, step=16), activation=\\'relu\\'))\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''    if hp.Boolean(\"dropout\"):\n",
    "        model.add(layers.Dropout(rate=0.25))\n",
    "    model.add(Dense(units=hp.Int('units_2',min_value=32, max_value=256, step=16), activation='relu'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2755a68",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bf57f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy Using Naive Bayes:  0.43166666666666664\n",
      "F1 Score: 0.376478265267993\n"
     ]
    }
   ],
   "source": [
    "#https://www.kaggle.com/code/mkowoods/deep-learning-lstm-for-tweet-classification/notebook#Winning-architecture-for-Quora-Challenge \n",
    "\n",
    "# Naive Bayse Baseline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "text_clf.fit(tokenizer.sequences_to_texts_generator(padded_sequence_tv), y_tv_array)\n",
    "predictions = text_clf.predict(tokenizer.sequences_to_texts_generator(padded_sequence_test)) \n",
    "print('Baseline Accuracy Using Naive Bayes: ', (predictions == y_test_array).mean())\n",
    "print('F1 Score:', f1_score(y_test_array, predictions, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e72a30",
   "metadata": {},
   "source": [
    "# 5. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad4b1476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 600, 128)          2423296   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,572,037\n",
      "Trainable params: 2,572,037\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/\n",
    "#https://keras.io/api/losses/\n",
    "# https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/categorical-crossentropy\n",
    "model = Sequential() \n",
    "model.add(Embedding(input_dim = vocab_size, output_dim = 128, input_length=600))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(5, activation='softmax')) \n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])  \n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e8363de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "113/113 [==============================] - 135s 1s/step - loss: 1.3554 - accuracy: 0.3801 - val_loss: 1.1675 - val_accuracy: 0.4594\n",
      "Epoch 2/3\n",
      "113/113 [==============================] - 131s 1s/step - loss: 0.9602 - accuracy: 0.5683 - val_loss: 1.1219 - val_accuracy: 0.5117\n",
      "Epoch 3/3\n",
      "113/113 [==============================] - 133s 1s/step - loss: 0.6974 - accuracy: 0.7149 - val_loss: 1.2924 - val_accuracy: 0.5028\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(padded_sequence_tv, y_tv, validation_data=(padded_sequence_test, y_test),\n",
    "                    epochs = 3,\n",
    "                    batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8221c6",
   "metadata": {},
   "source": [
    "# Forecast\n",
    "https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51fef157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       4\n",
       "1       2\n",
       "2       4\n",
       "3       5\n",
       "4       4\n",
       "       ..\n",
       "2995    3\n",
       "2996    4\n",
       "2997    1\n",
       "2998    5\n",
       "2999    4\n",
       "Length: 3000, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://keras.io/api/utils/python_utils/\n",
    "y_pred = model.predict(padded_sequence_realtest)\n",
    "df_pred = pd.DataFrame(y_pred, columns = [1,2,3,4,5])\n",
    "df_pred = df_pred.idxmax(axis=1)\n",
    "\n",
    "df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2336d040",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
